#!/usr/bin/env python
# Written by ChatGPT5

import torch
import torch.nn as nn
import torch.nn.functional as F
import my_utils

class CharTokenizer:
    def __init__(self):
        chars = [chr(i) for i in range(32, 127)]
        self.vocab = {c: i for i, c in enumerate(chars)}
        self.inv = {i: c for c, i in self.vocab.items()}
        self.pad_id = 0

    def encode(self, text):
        return [self.vocab.get(c, self.pad_id) for c in text]

    def decode(self, ids):
        return "".join(self.inv.get(i, "?") for i in ids)

class TinyAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.o_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        B, T, C = x.shape
        H = self.n_heads
        D = self.head_dim

        q = self.q_proj(x).view(B, T, H, D)
        k = self.k_proj(x).view(B, T, H, D)
        v = self.v_proj(x).view(B, T, H, D)

        att = (q @ k.transpose(-1, -2)) / (D ** 0.5)
        att = att.softmax(dim=-1)

        y = (att @ v).contiguous().view(B, T, C)
        y = self.o_proj(y)

        return y, att

class TinyTransformer(nn.Module):
    def __init__(self, vocab_size=128, d_model=64, n_heads=4, n_layers=4):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([
            TinyAttention(d_model, n_heads) for _ in range(n_layers)
        ])
        self.ln = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, idx):
        h = self.embed(idx)
        all_att = []

        for layer in self.layers:
            y, att = layer(h)
            h = h + y
            all_att.append(att)

        h = self.ln(h)
        logits = self.lm_head(h)
        return logits, all_att

def intro():
    my_utils.pretty_print("""
This was generated by ChatGPT5.
Emulating DeepSeek's TinyTransformer model,
This is a minimal transformer model that can be used to generate text.
""")

def main():
    intro()
    my_utils.wait_for_user_input()
    tokenizer = CharTokenizer()
    model = TinyTransformer()
    prompt = "Hello world!"
    ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long)
    logits, attentions = model(ids)
    print(f"=== Prompt: {prompt} ===")
    print("=== Input tokens ===")
    print(ids)
    print("\n=== Attention matrices (summary) ===")
    for i, att in enumerate(attentions):
        print(f"\nLayer {i}: shape {att.shape}")
        print("  Showing max attention value across all positions...")
        max_att = att[0].max().item()
        print("  Showing average attention...")
        avg_att = att[0].mean().item()
        print(f"  Max attention: {max_att:.4f}, Avg attention: {avg_att:.4f}")
        if att.shape[-1] <= 15:
            print("  Showing first token's attention to all other tokens (head 0)...")
            first_token_att = att[0, 0, 0, :].tolist()
            print(f"  First token attention (head 0): {[f'{x:.3f}' for x in first_token_att[:8]]}...")
    print("\n=== Output logits (per token) ===")
    print(logits)

if __name__ == "__main__":
    main()
